# Подсистема ИИ (анализ результатов нагрузочного тестирования)

Модуль `AI/` собирает метрики по доменам, строит компактные “контекст‑пакеты” и выполняет многошаговый анализ LLM. Результаты возвращаются как текст (для отображения) и как строгие структуры (JSON) для программной обработки и сохранения.

## Возможности
- Домены анализа: JVM, базы данных, Kafka, микросервисы, инфраструктурные ресурсы; опционально — `lt_framework` (метрики инструмента нагрузки).
- Контекст‑пакеты: резюме по сериям (средние/минимумы/максимумы/последние значения, время пиков, окна аномалий).
- Двухпроходный вывод: k кандидатов → приведение к строгому JSON → выбор лучшего по судье и верификации на данных.
- Вычисление “пиковой производительности” (peak_performance) в итоговом JSON при наличии соответствующих метрик.

## Как это работает (поток)
1) Сбор данных за интервал (PromQL/Flux/InfluxQL; напрямую или через Grafana‑proxy).  
2) Построение `context pack` по каждому домену.  
3) Генерация нескольких кандидатов ответов LLM и приведение к строгому JSON.  
4) Оценка кандидатов “судьёй” и проверкой на данных; выбор лучшего.  
5) Возврат: доменные текстовые блоки, общий итог, разобранные структуры (`*_parsed`) и метрики качества (`scores`).

Основная функция: `AI.pipeline.uploadFromLLM(start_ts, end_ts, ...)` (есть прокси в `AI.main.uploadFromLLM`).

## Настройка
Источник конфигурации — `settings.py`:
- `llm` — провайдер (`perplexity|openai|anthropic`), модель, лимиты токенов и таймауты;
- `metrics_source` — источник метрик: `type=prometheus|grafana_proxy`;
- `lt_metrics_source` — источник метрик инструмента нагрузки: `type=prometheus|grafana_proxy|influxdb`;
- `default_params` — шаг выборки и ресемплирование;
- `queries` — список запросов/разметки серий для каждого домена (PromQL/Flux/InfluxQL).
Для точечных изменений без перезапуска предназначен `settings_runtime.json` (в т.ч. по проектным областям: `per_area[<service>]`).  
Тексты промптов лежат в `AI/prompts/*.txt` и также могут переопределяться через рантайм‑конфиг.

## Формат результата (строгая часть)
Каждый домен и итоговый блок возвращают объект со следующими полями:
- `verdict` — один из: «Успешно», «Есть риски», «Провал», «Недостаточно данных»;
- `confidence` — уверенность модели в диапазоне [0..1];
- `findings` — список строк или объектов `{summary, severity, component, evidence}`;
- `recommended_actions` — список конкретных действий;
- `affected_components?` — необязательный список компонент;
- `peak_performance?` — необязательный блок `{max_rps, max_time, drop_time, method}` при наличии входных метрик общего RPS.

## Пример прямого вызова
```python
from AI.main import uploadFromLLM

start_ts = 1740126600  # UNIX-время (сек)
end_ts = 1740136200

res = uploadFromLLM(start_ts, end_ts, save_to_db=False)
print(res.keys())  # jvm, database, kafka, ms, hard_resources, lt_framework?, final, *_parsed, scores
```

## Отладка
- Пустые доменные блоки — проверьте доступность источников метрик и корректность временного интервала.
- Значения p95 для InfluxQL пустые — проверьте типы колонок и корректность запросов.
- Медленный ответ LLM — уменьшите `max_tokens`/параллельность, проверьте сеть и параметры провайдера.

Подробности по REST‑эндпоинтам и UI см. в корневом `README.md`.

